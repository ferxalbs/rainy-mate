// Rainy Cowork - AI Agent Service
// Orchestrates AI-driven file operations with natural language understanding
// Part of Phase 1: Core AI File Operations Engine

use crate::ai::AIProviderManager;
use crate::models::ProviderType;
use crate::services::file_operations::{
    ConflictStrategy, FileOpChange, FileOpType, FileOperationEngine, OrganizeStrategy,
    WorkspaceAnalysis,
};
use crate::services::{FileManager, SettingsManager};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tauri::ipc::Channel;
use tokio::sync::Mutex;
use uuid::Uuid;

// ============ Agent Event Types ============

/// Events emitted during agent task execution
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase", tag = "event", content = "data")]
pub enum AgentEvent {
    /// Task planning started
    PlanningStarted { task_id: String },
    /// Plan generated and ready for review
    PlanReady { task_id: String, plan: TaskPlan },
    /// Step execution started
    StepStarted {
        task_id: String,
        step_index: usize,
        description: String,
    },
    /// Step completed successfully
    StepCompleted {
        task_id: String,
        step_index: usize,
        changes: Vec<FileOpChange>,
    },
    /// Step failed
    StepFailed {
        task_id: String,
        step_index: usize,
        error: String,
    },
    /// Overall progress update
    Progress {
        task_id: String,
        progress: u8,
        message: String,
    },
    /// Task completed
    Completed { task_id: String, total_changes: u32 },
    /// Task failed
    Failed { task_id: String, error: String },
    /// Confirmation required
    ConfirmationRequired {
        task_id: String,
        message: String,
        affected_files: Vec<String>,
    },
}

// ============ Task Planning Types ============

/// Intent classification for user instruction
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "lowercase")]
pub enum TaskIntent {
    Question,
    Command,
}

/// Information about the AI model used for a task
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct ModelInfo {
    pub provider: String,
    pub model: String,
    pub plan_tier: String,
}

/// A complete task plan generated by the AI
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct TaskPlan {
    pub id: String,
    pub instruction: String,
    pub intent: TaskIntent,
    /// Direct answer for questions (None for commands)
    pub answer: Option<String>,
    /// Information about the AI model used
    pub model_used: Option<ModelInfo>,
    pub steps: Vec<PlannedStep>,
    pub estimated_changes: u32,
    pub requires_confirmation: bool,
    pub warnings: Vec<String>,
    pub created_at: DateTime<Utc>,
}

/// A single planned step
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase", tag = "type")]
pub enum PlannedStep {
    /// Create a new file
    CreateFile {
        path: String,
        content: String,
        description: String,
    },
    /// Modify existing file content
    ModifyFile {
        path: String,
        instruction: String,
        description: String,
    },
    /// Move file to new location
    MoveFile {
        source: String,
        destination: String,
        description: String,
    },
    /// Delete a file (to trash)
    DeleteFile { path: String, description: String },
    /// Organize folder contents
    OrganizeFolder {
        path: String,
        strategy: OrganizeStrategy,
        description: String,
    },
    /// Rename files with pattern
    BatchRename {
        files: Vec<String>,
        pattern: String,
        description: String,
    },
    /// Execute AI analysis
    AnalyzeContent {
        path: String,
        instruction: String,
        description: String,
    },
}

impl PlannedStep {
    pub fn description(&self) -> &str {
        match self {
            PlannedStep::CreateFile { description, .. } => description,
            PlannedStep::ModifyFile { description, .. } => description,
            PlannedStep::MoveFile { description, .. } => description,
            PlannedStep::DeleteFile { description, .. } => description,
            PlannedStep::OrganizeFolder { description, .. } => description,
            PlannedStep::BatchRename { description, .. } => description,
            PlannedStep::AnalyzeContent { description, .. } => description,
        }
    }

    pub fn is_destructive(&self) -> bool {
        matches!(
            self,
            PlannedStep::DeleteFile { .. } | PlannedStep::ModifyFile { .. }
        )
    }
}

/// Result of task execution
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct ExecutionResult {
    pub task_id: String,
    pub success: bool,
    pub total_steps: usize,
    pub completed_steps: usize,
    pub total_changes: u32,
    pub changes: Vec<FileOpChange>,
    pub errors: Vec<String>,
    pub duration_ms: u64,
}

/// Context about the workspace for AI understanding
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct WorkspaceContext {
    pub path: String,
    pub file_count: u64,
    pub folder_count: u64,
    pub file_types: Vec<String>,
    pub recent_files: Vec<String>,
}

// ============ AI Agent ============

/// AI-powered cowork agent for autonomous file operations
pub struct CoworkAgent {
    ai_provider: Arc<AIProviderManager>,
    file_ops: Arc<FileOperationEngine>,
    file_manager: Arc<FileManager>,
    settings: Arc<Mutex<SettingsManager>>,
    /// Pending plans awaiting confirmation
    pending_plans: tokio::sync::RwLock<HashMap<String, TaskPlan>>,
}

impl CoworkAgent {
    pub fn new(
        ai_provider: Arc<AIProviderManager>,
        file_ops: Arc<FileOperationEngine>,
        file_manager: Arc<FileManager>,
        settings: Arc<Mutex<SettingsManager>>,
    ) -> Self {
        Self {
            ai_provider,
            file_ops,
            file_manager,
            settings,
            pending_plans: tokio::sync::RwLock::new(HashMap::new()),
        }
    }

    /// Parse a natural language instruction into a structured task plan
    pub async fn parse_instruction(
        &self,
        instruction: &str,
        workspace_path: &str,
    ) -> Result<TaskPlan, String> {
        // Get workspace context
        let analysis = self
            .file_ops
            .analyze_workspace(workspace_path)
            .await
            .map_err(|e| e.to_string())?;

        let context = WorkspaceContext {
            path: workspace_path.to_string(),
            file_count: analysis.total_files,
            folder_count: analysis.total_folders,
            file_types: analysis.file_types.keys().cloned().collect(),
            recent_files: analysis
                .largest_files
                .iter()
                .take(5)
                .map(|f| f.name.clone())
                .collect(),
        };

        // Build AI prompt for task planning
        let prompt = self.build_planning_prompt(instruction, &context);

        // Smart provider selection with fallback
        let (ai_response, model_info) = self.execute_with_best_provider(&prompt).await?;

        // Parse AI response into TaskPlan
        let mut plan = self.parse_ai_response(&ai_response, instruction)?;

        // Add model attribution
        plan.model_used = Some(model_info);

        // Store pending plan
        self.pending_plans
            .write()
            .await
            .insert(plan.id.clone(), plan.clone());

        Ok(plan)
    }

    /// Execute prompt with the best available provider, respecting user preference
    async fn execute_with_best_provider(
        &self,
        prompt: &str,
    ) -> Result<(String, ModelInfo), String> {
        println!("ðŸ¤– AI Agent: execute_with_best_provider called");

        let selected_model = {
            let settings = self.settings.lock().await;
            settings.get_selected_model().to_string()
        };
        println!("ðŸŽ¯ Selected model from settings: '{}'", selected_model);

        // Check capabilities (refresh if needed)
        let caps = self.ai_provider.get_capabilities().await;

        println!(
            "ðŸ“Š AI Agent Selection: Model='{}', PlanPaid={}, AvailableCoworkModels={:?}, CanMakeRequest={}, PlanName='{}'",
            selected_model,
            caps.profile.plan.is_paid(),
            caps.models,
            caps.can_make_request(),
            caps.profile.plan.name
        );

        // Get available models to determine correct provider
        let available_models = crate::services::settings::SettingsManager::get_available_models(
            caps.profile.plan.is_paid(),
            &caps.models,
        );

        // Find the selected model in available models to get its provider
        let model_info = available_models.iter().find(|m| m.id == selected_model);
        
        if let Some(model) = model_info {
            println!("ðŸ“‹ Found model info: provider='{}', name='{}'", model.provider, model.name);
            
            // Route to correct provider based on model's provider field
            match model.provider.as_str() {
                "Cowork Subscription" => {
                    // Verify model is in caps.models and we can make requests
                    if caps.models.contains(&selected_model) && caps.can_make_request() {
                        match self.ai_provider
                            .execute_prompt(&ProviderType::CoworkApi, &selected_model, prompt, |_, _| {})
                            .await
                        {
                            Ok(response) => {
                                println!("âœ… AI Agent: Successfully used Cowork API for model '{}'", selected_model);
                                return Ok((
                                    response,
                                    ModelInfo {
                                        provider: "Cowork Subscription".to_string(),
                                        model: selected_model.to_string(),
                                        plan_tier: caps.profile.plan.name.clone(),
                                    },
                                ));
                            }
                            Err(e) => {
                                println!("âŒ AI Agent: Cowork model '{}' failed: {}", selected_model, e);
                            }
                        }
                    } else {
                        println!("âš ï¸ AI Agent: Cowork model '{}' not available in plan or no requests left", selected_model);
                    }
                }
                "Rainy API" => {
                    if self.ai_provider.has_api_key("rainy_api").await.unwrap_or(false) {
                        match self.ai_provider
                            .execute_prompt(&ProviderType::RainyApi, &selected_model, prompt, |_, _| {})
                            .await
                        {
                            Ok(response) => {
                                println!("âœ… AI Agent: Successfully used Rainy API for model '{}'", selected_model);
                                return Ok((
                                    response,
                                    ModelInfo {
                                        provider: "Rainy API".to_string(),
                                        model: selected_model.to_string(),
                                        plan_tier: "Pay-As-You-Go".to_string(),
                                    },
                                ));
                            }
                            Err(e) => {
                                println!("âŒ AI Agent: Rainy API model '{}' failed: {}", selected_model, e);
                            }
                        }
                    } else {
                        println!("âš ï¸ AI Agent: No Rainy API key configured");
                    }
                }
                "Google Gemini" => {
                    if self.ai_provider.has_api_key("gemini").await.unwrap_or(false) {
                        match self.ai_provider
                            .execute_prompt(&ProviderType::Gemini, &selected_model, prompt, |_, _| {})
                            .await
                        {
                            Ok(response) => {
                                println!("âœ… AI Agent: Successfully used Gemini BYOK for model '{}'", selected_model);
                                return Ok((
                                    response,
                                    ModelInfo {
                                        provider: "Google Gemini".to_string(),
                                        model: selected_model.to_string(),
                                        plan_tier: "BYOK".to_string(),
                                    },
                                ));
                            }
                            Err(e) => {
                                println!("âŒ AI Agent: Gemini BYOK model '{}' failed: {}", selected_model, e);
                            }
                        }
                    } else {
                        println!("âš ï¸ AI Agent: No Gemini API key configured");
                    }
                }
                _ => {
                    println!("âš ï¸ AI Agent: Unknown provider '{}' for model '{}'", model.provider, selected_model);
                }
            }
        } else {
            println!("âš ï¸ AI Agent: Selected model '{}' not found in available models", selected_model);
        }

        // ============ FALLBACK LOGIC ============
        // If selected model failed or wasn't applicable, use smart defaults

        // 1. Try Cowork default
        if caps.can_make_request() {
            let preferred_model = caps.models.first().map(|s| s.as_str()).unwrap_or("gpt-4o");
            if let Ok(response) = self.ai_provider
                .execute_prompt(&ProviderType::CoworkApi, preferred_model, prompt, |_, _| {})
                .await
            {
                return Ok((
                    response,
                    ModelInfo {
                        provider: "Cowork Subscription (Fallback)".to_string(),
                        model: preferred_model.to_string(),
                        plan_tier: caps.profile.plan.name.clone(),
                    },
                ));
            }
        }

        // 2. Try Rainy API default
        if self.ai_provider.has_api_key("rainy_api").await.unwrap_or(false) {
            if let Ok(response) = self.ai_provider
                .execute_prompt(&ProviderType::RainyApi, "gpt-4o", prompt, |_, _| {})
                .await
            {
                return Ok((
                    response,
                    ModelInfo {
                        provider: "Rainy API (Fallback)".to_string(),
                        model: "gpt-4o".to_string(),
                        plan_tier: "Pay-As-You-Go".to_string(),
                    },
                ));
            }
        }

        // 3. Fallback to Gemini (Free/BYOK)
        let gemini_model = "gemini-3-flash-high";
        let response = self.ai_provider
            .execute_prompt(&ProviderType::Gemini, gemini_model, prompt, |_, _| {})
            .await
            .map_err(|e| format!("All AI providers failed. Last error: {}", e))?;

        Ok((
            response,
            ModelInfo {
                provider: "Google Gemini".to_string(),
                model: gemini_model.to_string(),
                plan_tier: "Free / BYOK".to_string(),
            },
        ))
    }

    /// Build the prompt for AI task planning
    fn build_planning_prompt(&self, instruction: &str, context: &WorkspaceContext) -> String {
        format!(
            r#"You are an AI file management assistant for Rainy Cowork. Analyze the user's instruction and respond appropriately.

WORKSPACE INFO:
- Path: {}
- Files: {} | Folders: {}
- File types present: {}
- Sample files: {}

USER INSTRUCTION: "{}"

FIRST, classify the intent:
1. QUESTION - User is asking about files/folders (e.g., "What files are here?", "How many images?")
2. COMMAND - User wants to perform an operation (e.g., "Organize by type", "Delete old files")

Generate a JSON response with this structure:

For QUESTIONS:
{{
  "intent": "question",
  "answer": "Your detailed answer based on the workspace info above",
  "steps": []
}}

For COMMANDS:
{{
  "intent": "command",
  "answer": null,
  "steps": [
    {{
      "type": "organize_folder" | "move_file" | "delete_file" | "create_file" | "batch_rename",
      "path": "target path",
      "strategy": "by_type" | "by_date" | "by_extension" (for organize),
      "source": "source path" (for move),
      "destination": "dest path" (for move),
      "pattern": "rename pattern" (for batch_rename),
      "files": ["file1", "file2"] (for batch_rename),
      "content": "file content" (for create),
      "description": "Human readable description of this step"
    }}
  ],
  "warnings": ["any important warnings"],
  "requires_confirmation": true/false (true if any destructive operations)
}}

RULES:
1. For questions, provide helpful answers using the workspace info
2. For commands, only suggest operations that match the user's intent
3. Set requires_confirmation=true for any delete or modify operations
4. Include clear descriptions for each step
5. If unsure whether it's a question or command, treat it as a question
6. For "organize", prefer "by_type" strategy unless user specifies otherwise

Respond ONLY with valid JSON, no other text."#,
            context.path,
            context.file_count,
            context.folder_count,
            context.file_types.join(", "),
            context.recent_files.join(", "),
            instruction
        )
    }

    /// Parse AI response into a TaskPlan
    fn parse_ai_response(&self, response: &str, instruction: &str) -> Result<TaskPlan, String> {
        // Extract JSON from response (handle markdown code blocks)
        let json_str = if response.contains("```json") {
            response
                .split("```json")
                .nth(1)
                .and_then(|s| s.split("```").next())
                .unwrap_or(response)
        } else if response.contains("```") {
            response.split("```").nth(1).unwrap_or(response)
        } else {
            response
        };

        // Parse JSON
        let parsed: serde_json::Value = serde_json::from_str(json_str.trim())
            .map_err(|e| format!("Failed to parse AI response: {}", e))?;

        // Parse intent (default to command for backwards compatibility)
        let intent = parsed
            .get("intent")
            .and_then(|i| i.as_str())
            .map(|s| match s {
                "question" => TaskIntent::Question,
                _ => TaskIntent::Command,
            })
            .unwrap_or(TaskIntent::Command);

        // Parse answer for questions
        let answer = parsed
            .get("answer")
            .and_then(|a| a.as_str())
            .map(|s| s.to_string());

        let mut steps = Vec::new();
        let mut warnings = Vec::new();
        let mut requires_confirmation = false;

        // Parse steps (only for commands)
        if intent == TaskIntent::Command {
            if let Some(steps_array) = parsed.get("steps").and_then(|s| s.as_array()) {
                for step_value in steps_array {
                    if let Some(step) = self.parse_step(step_value) {
                        if step.is_destructive() {
                            requires_confirmation = true;
                        }
                        steps.push(step);
                    }
                }
            }
        }

        // Parse warnings
        if let Some(warnings_array) = parsed.get("warnings").and_then(|w| w.as_array()) {
            for warning in warnings_array {
                if let Some(w) = warning.as_str() {
                    warnings.push(w.to_string());
                }
            }
        }

        // Override confirmation if explicitly set
        if let Some(confirm) = parsed
            .get("requires_confirmation")
            .and_then(|c| c.as_bool())
        {
            requires_confirmation = confirm;
        }

        Ok(TaskPlan {
            id: Uuid::new_v4().to_string(),
            instruction: instruction.to_string(),
            intent,
            answer,
            model_used: None, // Will be set by caller with actual model info
            steps: steps.clone(),
            estimated_changes: steps.len() as u32,
            requires_confirmation,
            warnings,
            created_at: Utc::now(),
        })
    }

    /// Parse a single step from JSON
    fn parse_step(&self, value: &serde_json::Value) -> Option<PlannedStep> {
        let step_type = value.get("type")?.as_str()?;
        let description = value
            .get("description")
            .and_then(|d| d.as_str())
            .unwrap_or("Execute operation")
            .to_string();

        match step_type {
            "organize_folder" => {
                let path = value.get("path")?.as_str()?.to_string();
                let strategy_str = value
                    .get("strategy")
                    .and_then(|s| s.as_str())
                    .unwrap_or("by_type");

                let strategy = match strategy_str {
                    "by_date" => OrganizeStrategy::ByDate,
                    "by_extension" => OrganizeStrategy::ByExtension,
                    "by_content" => OrganizeStrategy::ByContent,
                    _ => OrganizeStrategy::ByType,
                };

                Some(PlannedStep::OrganizeFolder {
                    path,
                    strategy,
                    description,
                })
            }
            "move_file" => {
                let source = value.get("source")?.as_str()?.to_string();
                let destination = value.get("destination")?.as_str()?.to_string();
                Some(PlannedStep::MoveFile {
                    source,
                    destination,
                    description,
                })
            }
            "delete_file" => {
                let path = value.get("path")?.as_str()?.to_string();
                Some(PlannedStep::DeleteFile { path, description })
            }
            "create_file" => {
                let path = value.get("path")?.as_str()?.to_string();
                let content = value
                    .get("content")
                    .and_then(|c| c.as_str())
                    .unwrap_or("")
                    .to_string();
                Some(PlannedStep::CreateFile {
                    path,
                    content,
                    description,
                })
            }
            "batch_rename" => {
                let files: Vec<String> = value
                    .get("files")?
                    .as_array()?
                    .iter()
                    .filter_map(|f| f.as_str().map(|s| s.to_string()))
                    .collect();
                let pattern = value.get("pattern")?.as_str()?.to_string();
                Some(PlannedStep::BatchRename {
                    files,
                    pattern,
                    description,
                })
            }
            _ => None,
        }
    }

    /// Execute a planned task
    pub async fn execute_plan(
        &self,
        plan_id: &str,
        on_event: Channel<AgentEvent>,
    ) -> Result<ExecutionResult, String> {
        let start_time = std::time::Instant::now();

        // Get the plan
        let plan = self
            .pending_plans
            .read()
            .await
            .get(plan_id)
            .cloned()
            .ok_or_else(|| format!("Plan not found: {}", plan_id))?;

        let task_id = plan_id.to_string();
        let total_steps = plan.steps.len();
        let mut completed_steps = 0;
        let mut total_changes = 0u32;
        let mut all_changes = Vec::new();
        let mut errors = Vec::new();

        // Execute each step
        for (index, step) in plan.steps.iter().enumerate() {
            // Emit step started
            let _ = on_event.send(AgentEvent::StepStarted {
                task_id: task_id.clone(),
                step_index: index,
                description: step.description().to_string(),
            });

            // Execute the step
            match self.execute_step(step).await {
                Ok(changes) => {
                    total_changes += changes.len() as u32;
                    all_changes.extend(changes.clone());
                    completed_steps += 1;

                    let _ = on_event.send(AgentEvent::StepCompleted {
                        task_id: task_id.clone(),
                        step_index: index,
                        changes,
                    });
                }
                Err(e) => {
                    errors.push(format!("Step {}: {}", index + 1, e));
                    let _ = on_event.send(AgentEvent::StepFailed {
                        task_id: task_id.clone(),
                        step_index: index,
                        error: e,
                    });
                }
            }

            // Update progress
            let progress = ((index + 1) as f32 / total_steps as f32 * 100.0) as u8;
            let _ = on_event.send(AgentEvent::Progress {
                task_id: task_id.clone(),
                progress,
                message: format!("Step {} of {} complete", index + 1, total_steps),
            });
        }

        // Remove from pending
        self.pending_plans.write().await.remove(plan_id);

        let duration_ms = start_time.elapsed().as_millis() as u64;
        let success = errors.is_empty();

        // Emit completion event
        if success {
            let _ = on_event.send(AgentEvent::Completed {
                task_id: task_id.clone(),
                total_changes,
            });
        } else {
            let _ = on_event.send(AgentEvent::Failed {
                task_id: task_id.clone(),
                error: errors.join("; "),
            });
        }

        Ok(ExecutionResult {
            task_id,
            success,
            total_steps,
            completed_steps,
            total_changes,
            changes: all_changes,
            errors,
            duration_ms,
        })
    }

    /// Execute a single step
    async fn execute_step(&self, step: &PlannedStep) -> Result<Vec<FileOpChange>, String> {
        match step {
            PlannedStep::OrganizeFolder { path, strategy, .. } => {
                let result = self
                    .file_ops
                    .organize_folder(path, strategy.clone(), false)
                    .await
                    .map_err(|e| e.to_string())?;
                Ok(result.changes)
            }
            PlannedStep::MoveFile {
                source,
                destination,
                ..
            } => {
                use crate::services::file_operations::MoveOperation;
                let ops = vec![MoveOperation {
                    source: source.clone(),
                    destination: destination.clone(),
                    on_conflict: ConflictStrategy::Rename,
                }];
                self.file_ops
                    .move_files(ops)
                    .await
                    .map_err(|e| e.to_string())
            }
            PlannedStep::DeleteFile { path, .. } => self
                .file_ops
                .safe_delete(vec![path.clone()])
                .await
                .map_err(|e| e.to_string()),
            PlannedStep::CreateFile { path, content, .. } => {
                // Use file manager to write
                let change = self
                    .file_manager
                    .write_file(path, content, None)
                    .await
                    .map_err(|e| e)?;

                Ok(vec![FileOpChange {
                    id: change.id,
                    operation: FileOpType::Create,
                    source_path: path.clone(),
                    dest_path: None,
                    timestamp: Utc::now(),
                    reversible: true,
                }])
            }
            PlannedStep::BatchRename { files, pattern, .. } => {
                use crate::services::file_operations::RenamePattern;
                let rename_pattern = RenamePattern {
                    template: pattern.clone(),
                    find: None,
                    replace: None,
                    counter_start: Some(1),
                    counter_padding: Some(3),
                };

                let previews = self
                    .file_ops
                    .batch_rename(files.clone(), rename_pattern, false)
                    .await
                    .map_err(|e| e.to_string())?;

                // Convert previews to changes
                let changes: Vec<FileOpChange> = previews
                    .into_iter()
                    .filter(|p| !p.has_conflict)
                    .map(|p| FileOpChange {
                        id: Uuid::new_v4().to_string(),
                        operation: FileOpType::Rename,
                        source_path: p.original,
                        dest_path: Some(p.new_name),
                        timestamp: Utc::now(),
                        reversible: true,
                    })
                    .collect();

                Ok(changes)
            }
            PlannedStep::ModifyFile {
                path, instruction, ..
            } => {
                // Read current content
                let current = self.file_manager.read_file(path).await.map_err(|e| e)?;

                // Use AI to transform content
                let new_content = {
                    let prompt = format!(
                        "Modify this file content according to the instruction.\n\nINSTRUCTION: {}\n\nCURRENT CONTENT:\n{}\n\nRespond with ONLY the new file content, nothing else.",
                        instruction, current
                    );

                    self.ai_provider
                        .execute_prompt(&ProviderType::Gemini, "gemini-2.5-flash", &prompt, |_, _| {})
                        .await
                        .map_err(|e| e.to_string())?
                };

                // Write back
                let change = self
                    .file_manager
                    .write_file(path, &new_content, None)
                    .await?;

                Ok(vec![FileOpChange {
                    id: change.id,
                    operation: FileOpType::Move, // Using Move as "Modify" equivalent
                    source_path: path.clone(),
                    dest_path: None,
                    timestamp: Utc::now(),
                    reversible: true,
                }])
            }
            PlannedStep::AnalyzeContent { .. } => {
                // Analysis doesn't produce file changes
                Ok(Vec::new())
            }
        }
    }

    /// Get a pending plan by ID
    pub async fn get_plan(&self, plan_id: &str) -> Option<TaskPlan> {
        self.pending_plans.read().await.get(plan_id).cloned()
    }

    /// Cancel a pending plan
    pub async fn cancel_plan(&self, plan_id: &str) -> Result<(), String> {
        self.pending_plans
            .write()
            .await
            .remove(plan_id)
            .map(|_| ())
            .ok_or_else(|| format!("Plan not found: {}", plan_id))
    }

    /// Analyze workspace and generate suggestions
    pub async fn analyze_workspace(&self, path: &str) -> Result<WorkspaceAnalysis, String> {
        self.file_ops
            .analyze_workspace(path)
            .await
            .map_err(|e| e.to_string())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_step_destructive_check() {
        let delete_step = PlannedStep::DeleteFile {
            path: "/test".to_string(),
            description: "Delete".to_string(),
        };
        assert!(delete_step.is_destructive());

        let move_step = PlannedStep::MoveFile {
            source: "/a".to_string(),
            destination: "/b".to_string(),
            description: "Move".to_string(),
        };
        assert!(!move_step.is_destructive());
    }
}
