// Rainy Cowork - AI Agent Service
// Orchestrates AI-driven file operations with natural language understanding
// Part of Phase 1: Core AI File Operations Engine

use crate::ai::provider_types::StreamingChunk;
use crate::ai::AIProviderManager;
use crate::models::ProviderType;
use crate::services::file_operations::{
    ConflictStrategy, FileOpChange, FileOpType, FileOperationEngine, OrganizeStrategy,
    WorkspaceAnalysis,
};
use crate::services::{FileManager, SettingsManager};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tauri::ipc::Channel;
use tokio::sync::Mutex;
use uuid::Uuid;

// ============ Agent Event Types ============

/// Events emitted during agent task execution
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase", tag = "event", content = "data")]
pub enum AgentEvent {
    /// Task planning started
    PlanningStarted { task_id: String },
    /// Plan generated and ready for review
    PlanReady { task_id: String, plan: TaskPlan },
    /// Step execution started
    StepStarted {
        task_id: String,
        step_index: usize,
        description: String,
    },
    /// Step completed successfully
    StepCompleted {
        task_id: String,
        step_index: usize,
        changes: Vec<FileOpChange>,
    },
    /// Step failed
    StepFailed {
        task_id: String,
        step_index: usize,
        error: String,
    },
    /// Overall progress update
    Progress {
        task_id: String,
        progress: u8,
        message: String,
    },
    /// Task completed
    Completed { task_id: String, total_changes: u32 },
    /// Task failed
    Failed { task_id: String, error: String },
    /// Confirmation required
    ConfirmationRequired {
        task_id: String,
        message: String,
        affected_files: Vec<String>,
    },
}

// ============ Task Planning Types ============

/// Intent classification for user instruction
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "lowercase")]
pub enum TaskIntent {
    Question,
    Command,
}

/// Information about the AI model used for a task
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct ModelInfo {
    pub provider: String,
    pub model: String,
    pub plan_tier: String,
}

/// A complete task plan generated by the AI
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct TaskPlan {
    pub id: String,
    pub instruction: String,
    pub intent: TaskIntent,
    /// Direct answer for questions (None for commands)
    pub answer: Option<String>,
    /// Thinking process (if available)
    pub thought: Option<String>,
    /// Information about the AI model used
    pub model_used: Option<ModelInfo>,
    pub steps: Vec<PlannedStep>,
    pub estimated_changes: u32,
    pub requires_confirmation: bool,
    pub warnings: Vec<String>,
    pub created_at: DateTime<Utc>,
}

/// A single planned step
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase", tag = "type")]
pub enum PlannedStep {
    /// Create a new file
    CreateFile {
        path: String,
        content: String,
        description: String,
    },
    /// Modify existing file content
    ModifyFile {
        path: String,
        instruction: String,
        description: String,
    },
    /// Move file to new location
    MoveFile {
        source: String,
        destination: String,
        description: String,
    },
    /// Delete a file (to trash)
    DeleteFile { path: String, description: String },
    /// Organize folder contents
    OrganizeFolder {
        path: String,
        strategy: OrganizeStrategy,
        description: String,
    },
    /// Rename files with pattern
    BatchRename {
        files: Vec<String>,
        pattern: String,
        description: String,
    },
    /// Execute AI analysis
    AnalyzeContent {
        path: String,
        instruction: String,
        description: String,
    },
}

impl PlannedStep {
    pub fn description(&self) -> &str {
        match self {
            PlannedStep::CreateFile { description, .. } => description,
            PlannedStep::ModifyFile { description, .. } => description,
            PlannedStep::MoveFile { description, .. } => description,
            PlannedStep::DeleteFile { description, .. } => description,
            PlannedStep::OrganizeFolder { description, .. } => description,
            PlannedStep::BatchRename { description, .. } => description,
            PlannedStep::AnalyzeContent { description, .. } => description,
        }
    }

    pub fn is_destructive(&self) -> bool {
        matches!(
            self,
            PlannedStep::DeleteFile { .. } | PlannedStep::ModifyFile { .. }
        )
    }
}

/// Result of task execution
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct ExecutionResult {
    pub task_id: String,
    pub success: bool,
    pub total_steps: usize,
    pub completed_steps: usize,
    pub total_changes: u32,
    pub changes: Vec<FileOpChange>,
    pub errors: Vec<String>,
    pub duration_ms: u64,
}

/// Context about the workspace for AI understanding
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct WorkspaceContext {
    pub path: String,
    pub file_count: u64,
    pub folder_count: u64,
    pub file_types: Vec<String>,
    pub recent_files: Vec<String>,
}

// ============ AI Agent ============

/// AI-powered cowork agent for autonomous file operations
pub struct CoworkAgent {
    ai_provider: Arc<AIProviderManager>,
    file_ops: Arc<FileOperationEngine>,
    file_manager: Arc<FileManager>,
    settings: Arc<Mutex<SettingsManager>>,
    /// Pending plans awaiting confirmation
    pending_plans: tokio::sync::RwLock<HashMap<String, TaskPlan>>,
}

impl CoworkAgent {
    pub fn new(
        ai_provider: Arc<AIProviderManager>,
        file_ops: Arc<FileOperationEngine>,
        file_manager: Arc<FileManager>,
        settings: Arc<Mutex<SettingsManager>>,
    ) -> Self {
        Self {
            ai_provider,
            file_ops,
            file_manager,
            settings,
            pending_plans: tokio::sync::RwLock::new(HashMap::new()),
        }
    }

    /// Parse a natural language instruction into a structured task plan
    pub async fn parse_instruction(
        &self,
        instruction: &str,
        workspace_path: &str,
    ) -> Result<TaskPlan, String> {
        // Create and set workspace context for file operations
        // This is an ad-hoc workspace based on the user's selected folder
        let adhoc_workspace = crate::services::workspace::Workspace {
            id: uuid::Uuid::new_v4(),
            name: format!("Cowork - {}", workspace_path),
            allowed_paths: vec![workspace_path.to_string()],
            permissions: crate::services::workspace::WorkspacePermissions {
                can_read: true,
                can_write: true,
                can_execute: false,
                can_delete: true, // Allow delete for organize/cleanup operations
                can_create_agents: false,
            },
            permission_overrides: Vec::new(),
            agents: Vec::new(),
            memory: crate::services::workspace::WorkspaceMemory {
                max_size: 1024 * 1024 * 100, // 100MB
                current_size: 0,
                retention_policy: "fifo".to_string(),
            },
            settings: crate::services::workspace::WorkspaceSettings {
                theme: "default".to_string(),
                language: "en".to_string(),
                auto_save: true,
                notifications_enabled: true,
            },
        };

        // Set the workspace context on file operations engine
        self.file_ops.set_workspace(adhoc_workspace).await;

        // Get workspace context
        let analysis = self
            .file_ops
            .analyze_workspace(workspace_path)
            .await
            .map_err(|e| e.to_string())?;

        let context = WorkspaceContext {
            path: workspace_path.to_string(),
            file_count: analysis.total_files,
            folder_count: analysis.total_folders,
            file_types: analysis.file_types.keys().cloned().collect(),
            recent_files: analysis
                .largest_files
                .iter()
                .take(5)
                .map(|f| f.name.clone())
                .collect(),
        };

        // Build AI prompt for task planning
        let prompt = self.build_planning_prompt(instruction, &context);

        // Smart provider selection with fallback
        let (ai_response, model_info, thought) = self.execute_with_best_provider(&prompt).await?;

        // Parse AI response into TaskPlan
        let mut plan = self.parse_ai_response(&ai_response, instruction)?;

        // Add model attribution and thought
        plan.model_used = Some(model_info);
        plan.thought = thought;

        // Store pending plan
        self.pending_plans
            .write()
            .await
            .insert(plan.id.clone(), plan.clone());

        Ok(plan)
    }

    /// Execute prompt with the best available provider, respecting user preference
    async fn execute_with_best_provider(
        &self,
        prompt: &str,
    ) -> Result<(String, ModelInfo, Option<String>), String> {
        println!("ü§ñ AI Agent: execute_with_best_provider called");

        let selected_model = {
            let settings = self.settings.lock().await;
            settings.get_selected_model().to_string()
        };
        println!("üéØ Selected model from settings: '{}'", selected_model);

        // Check capabilities (refresh if needed)
        let caps = self.ai_provider.get_capabilities().await;

        println!(
            "üìä AI Agent Selection: Model='{}', PlanPaid={}, AvailableCoworkModels={:?}, CanMakeRequest={}, PlanName='{}'",
            selected_model,
            caps.profile.plan.is_paid(),
            caps.models,
            caps.can_make_request(),
            caps.profile.plan.name
        );

        // Get available models to determine correct provider
        let available_models = crate::services::settings::SettingsManager::get_available_models(
            caps.profile.plan.is_paid(),
            &caps.models,
        );

        println!("üìã Available models count: {}", available_models.len());
        println!(
            "üìã Available models: {:?}",
            available_models.iter().map(|m| &m.id).collect::<Vec<_>>()
        );

        // Strip provider prefix from selected_model for lookup
        // The settings store model IDs with prefixes like "rainy:gemini-3-flash-minimal"
        // but available_models uses raw IDs without prefixes
        let raw_model_id = if selected_model.starts_with("rainy:") {
            selected_model
                .strip_prefix("rainy:")
                .unwrap_or(&selected_model)
        } else if selected_model.starts_with("cowork:") {
            selected_model
                .strip_prefix("cowork:")
                .unwrap_or(&selected_model)
        } else if selected_model.starts_with("gemini:") {
            selected_model
                .strip_prefix("gemini:")
                .unwrap_or(&selected_model)
        } else {
            &selected_model
        };

        println!("üîç Raw model ID for lookup: '{}'", raw_model_id);

        // Find the selected model in available models to get its provider
        let model_info = available_models.iter().find(|m| m.id == raw_model_id);

        if let Some(model) = model_info {
            println!(
                "üìã Found model info: provider='{}', name='{}', available={}",
                model.provider, model.name, model.is_available
            );

            // CRITICAL: Route based on MODEL ID PREFIX first, then fall back to model.provider
            // The prefix in selected_model (rainy:, cowork:, gemini:) takes priority!
            let effective_provider = if selected_model.starts_with("rainy:") {
                "Rainy API" // Force Rainy API routing for rainy: prefix
            } else if selected_model.starts_with("cowork:") {
                "Cowork Subscription" // Force Cowork routing for cowork: prefix
            } else if selected_model.starts_with("gemini:") {
                "Google Gemini" // Force Gemini BYOK for gemini: prefix
            } else {
                model.provider.as_str() // Fall back to model's provider field
            };

            println!(
                "üéØ Effective provider (prefix-priority): '{}'",
                effective_provider
            );

            // Route to correct provider based on effective provider
            match effective_provider {
                "Cowork" | "Cowork Subscription" => {
                    // Verify model is in caps.models and we can make requests
                    if caps.models.contains(&selected_model) && caps.can_make_request() {
                        println!(
                            "‚úÖ Model '{}' is in caps.models and can make request",
                            selected_model
                        );
                        let thought_acc = std::sync::Arc::new(std::sync::Mutex::new(String::new()));
                        let thought_writer = thought_acc.clone();

                        match self
                            .ai_provider
                            .execute_prompt(
                                &ProviderType::CoworkApi,
                                &model.name,
                                prompt,
                                |_, _| {},
                                Some(move |chunk: StreamingChunk| {
                                    if let Some(t) = chunk.thought {
                                        if let Ok(mut g) = thought_writer.lock() {
                                            g.push_str(&t);
                                        }
                                    }
                                }),
                            )
                            .await
                        {
                            Ok(response) => {
                                println!(
                                    "‚úÖ AI Agent: Successfully used Cowork API for model '{}'",
                                    selected_model
                                );
                                let thought = {
                                    let g = thought_acc.lock().unwrap();
                                    if g.is_empty() {
                                        None
                                    } else {
                                        Some(g.clone())
                                    }
                                };
                                return Ok((
                                    response,
                                    ModelInfo {
                                        provider: "Cowork Subscription".to_string(),
                                        model: model.name.clone(),
                                        plan_tier: caps.profile.plan.name.clone(),
                                    },
                                    thought,
                                ));
                            }
                            Err(e) => {
                                println!(
                                    "‚ùå AI Agent: Cowork model '{}' failed: {}",
                                    selected_model, e
                                );
                            }
                        }
                    } else {
                        if !caps.models.contains(&selected_model) {
                            println!(
                                "‚ö†Ô∏è AI Agent: Model '{}' NOT in caps.models. Available: {:?}",
                                selected_model, caps.models
                            );
                        }
                        if !caps.can_make_request() {
                            println!(
                                "‚ö†Ô∏è AI Agent: Cannot make request. Used: {}/{}",
                                caps.profile.usage.used, caps.profile.usage.limit
                            );
                        }
                    }
                }
                "Rainy API" => {
                    if self
                        .ai_provider
                        .has_api_key("rainy_api")
                        .await
                        .unwrap_or(false)
                    {
                        let thought_acc = std::sync::Arc::new(std::sync::Mutex::new(String::new()));
                        let thought_writer = thought_acc.clone();

                        match self
                            .ai_provider
                            .execute_prompt(
                                &ProviderType::RainyApi,
                                raw_model_id, // Use raw model ID, NOT model.name (display name)!
                                prompt,
                                |_, _| {},
                                Some(move |chunk: StreamingChunk| {
                                    if let Some(t) = chunk.thought {
                                        if let Ok(mut g) = thought_writer.lock() {
                                            g.push_str(&t);
                                        }
                                    }
                                }),
                            )
                            .await
                        {
                            Ok(response) => {
                                println!(
                                    "‚úÖ AI Agent: Successfully used Rainy API for model '{}'",
                                    selected_model
                                );
                                let thought = {
                                    let g = thought_acc.lock().unwrap();
                                    if g.is_empty() {
                                        None
                                    } else {
                                        Some(g.clone())
                                    }
                                };
                                return Ok((
                                    response,
                                    ModelInfo {
                                        provider: "Rainy API".to_string(),
                                        model: model.name.clone(),
                                        plan_tier: "Pay-As-You-Go".to_string(),
                                    },
                                    thought,
                                ));
                            }
                            Err(e) => {
                                println!(
                                    "‚ùå AI Agent: Rainy API model '{}' failed: {}",
                                    selected_model, e
                                );
                            }
                        }
                    } else {
                        println!("‚ö†Ô∏è AI Agent: No Rainy API key configured");
                    }
                }
                "Google Gemini" => {
                    if self
                        .ai_provider
                        .has_api_key("gemini")
                        .await
                        .unwrap_or(false)
                    {
                        let thought_acc = std::sync::Arc::new(std::sync::Mutex::new(String::new()));
                        let thought_writer = thought_acc.clone();

                        match self
                            .ai_provider
                            .execute_prompt(
                                &ProviderType::Gemini,
                                &model.name,
                                prompt,
                                |_, _| {},
                                Some(move |chunk: StreamingChunk| {
                                    if let Some(t) = chunk.thought {
                                        if let Ok(mut g) = thought_writer.lock() {
                                            g.push_str(&t);
                                        }
                                    }
                                }),
                            )
                            .await
                        {
                            Ok(response) => {
                                println!(
                                    "‚úÖ AI Agent: Successfully used Gemini BYOK for model '{}'",
                                    selected_model
                                );
                                let thought = {
                                    let g = thought_acc.lock().unwrap();
                                    if g.is_empty() {
                                        None
                                    } else {
                                        Some(g.clone())
                                    }
                                };
                                return Ok((
                                    response,
                                    ModelInfo {
                                        provider: "Google Gemini".to_string(),
                                        model: model.name.clone(),
                                        plan_tier: "BYOK".to_string(),
                                    },
                                    thought,
                                ));
                            }
                            Err(e) => {
                                println!(
                                    "‚ùå AI Agent: Gemini BYOK model '{}' failed: {}",
                                    selected_model, e
                                );
                            }
                        }
                    } else {
                        println!("‚ö†Ô∏è AI Agent: No Gemini API key configured");
                    }
                }
                _ => {
                    println!(
                        "‚ö†Ô∏è AI Agent: Unknown provider '{}' for model '{}'",
                        model.provider, selected_model
                    );
                }
            }
        } else {
            println!(
                "‚ö†Ô∏è AI Agent: Selected model '{}' not found in available models (raw_id='{}')",
                selected_model, raw_model_id
            );

            // Determine provider from prefix and route directly
            if selected_model.starts_with("rainy:") {
                println!("üîç Detected Rainy API model from prefix, attempting direct execution");
                if self
                    .ai_provider
                    .has_api_key("rainy_api")
                    .await
                    .unwrap_or(false)
                {
                    match self
                        .ai_provider
                        .execute_prompt(
                            &ProviderType::RainyApi,
                            raw_model_id,
                            prompt,
                            |_, _| {},
                            None::<fn(StreamingChunk)>,
                        )
                        .await
                    {
                        Ok(response) => {
                            println!(
                                "‚úÖ AI Agent: Successfully used Rainy API for model '{}'",
                                raw_model_id
                            );
                            return Ok((
                                response,
                                ModelInfo {
                                    provider: "Rainy API".to_string(),
                                    model: raw_model_id.to_string(),
                                    plan_tier: "Pay-As-You-Go".to_string(),
                                },
                                None,
                            ));
                        }
                        Err(e) => {
                            println!("‚ùå AI Agent: Rainy API direct execution failed: {}", e);
                        }
                    }
                } else {
                    println!("‚ö†Ô∏è AI Agent: No Rainy API key configured");
                }
            } else if selected_model.starts_with("cowork:") {
                println!("üîç Detected Cowork model from prefix, attempting direct execution");
                if caps.can_make_request() {
                    match self
                        .ai_provider
                        .execute_prompt(
                            &ProviderType::CoworkApi,
                            raw_model_id,
                            prompt,
                            |_, _| {},
                            None::<fn(StreamingChunk)>,
                        )
                        .await
                    {
                        Ok(response) => {
                            println!(
                                "‚úÖ AI Agent: Successfully used Cowork API for model '{}'",
                                raw_model_id
                            );
                            return Ok((
                                response,
                                ModelInfo {
                                    provider: "Cowork Subscription".to_string(),
                                    model: raw_model_id.to_string(),
                                    plan_tier: caps.profile.plan.name.clone(),
                                },
                                None,
                            ));
                        }
                        Err(e) => {
                            println!("‚ùå AI Agent: Cowork API direct execution failed: {}", e);
                        }
                    }
                }
            } else if selected_model.starts_with("gemini:") || raw_model_id.starts_with("gemini") {
                println!("üîç Detected Gemini BYOK model, attempting direct execution");
                if self
                    .ai_provider
                    .has_api_key("gemini")
                    .await
                    .unwrap_or(false)
                {
                    match self
                        .ai_provider
                        .execute_prompt(
                            &ProviderType::Gemini,
                            raw_model_id,
                            prompt,
                            |_, _| {},
                            None::<fn(StreamingChunk)>,
                        )
                        .await
                    {
                        Ok(response) => {
                            println!(
                                "‚úÖ AI Agent: Successfully used Gemini BYOK for model '{}'",
                                raw_model_id
                            );
                            return Ok((
                                response,
                                ModelInfo {
                                    provider: "Google Gemini".to_string(),
                                    model: raw_model_id.to_string(),
                                    plan_tier: "BYOK".to_string(),
                                },
                                None,
                            ));
                        }
                        Err(e) => {
                            println!("‚ùå AI Agent: Gemini BYOK attempt failed: {}", e);
                        }
                    }
                } else {
                    println!("‚ö†Ô∏è AI Agent: No Gemini API key configured");
                }
            }
        }

        // ============ FALLBACK LOGIC ============
        // If selected model failed or wasn't applicable, use smart defaults
        println!("üîÑ Entering fallback logic");

        // 1. Try Cowork default (first available model)
        if caps.can_make_request() && !caps.models.is_empty() {
            let preferred_model = caps.models.first().unwrap();
            println!("üîÑ Trying Cowork fallback with model: {}", preferred_model);

            let thought_acc = std::sync::Arc::new(std::sync::Mutex::new(String::new()));
            let thought_writer = thought_acc.clone();

            if let Ok(response) = self
                .ai_provider
                .execute_prompt(
                    &ProviderType::CoworkApi,
                    preferred_model,
                    prompt,
                    |_, _| {},
                    Some(move |chunk: StreamingChunk| {
                        if let Some(t) = chunk.thought {
                            if let Ok(mut g) = thought_writer.lock() {
                                g.push_str(&t);
                            }
                        }
                    }),
                )
                .await
            {
                println!("‚úÖ Cowork fallback successful");
                let thought = {
                    let g = thought_acc.lock().unwrap();
                    if g.is_empty() {
                        None
                    } else {
                        Some(g.clone())
                    }
                };
                return Ok((
                    response,
                    ModelInfo {
                        provider: "Cowork Subscription (Fallback)".to_string(),
                        model: preferred_model.to_string(),
                        plan_tier: caps.profile.plan.name.clone(),
                    },
                    thought,
                ));
            } else {
                println!("‚ùå Cowork fallback failed");
            }
        } else {
            println!(
                "‚ö†Ô∏è Cannot use Cowork fallback: can_make_request={}, models_count={}",
                caps.can_make_request(),
                caps.models.len()
            );
        }

        // 2. Try Rainy API default
        if self
            .ai_provider
            .has_api_key("rainy_api")
            .await
            .unwrap_or(false)
        {
            let thought_acc = std::sync::Arc::new(std::sync::Mutex::new(String::new()));
            let thought_writer = thought_acc.clone();
            println!("üîÑ Trying Rainy API fallback");
            if let Ok(response) = self
                .ai_provider
                .execute_prompt(
                    &ProviderType::RainyApi,
                    "gemini-2.5-flash",
                    prompt,
                    |_, _| {},
                    Some(move |chunk: StreamingChunk| {
                        if let Some(t) = chunk.thought {
                            if let Ok(mut g) = thought_writer.lock() {
                                g.push_str(&t);
                            }
                        }
                    }),
                )
                .await
            {
                println!("‚úÖ Rainy API fallback successful");
                let thought = {
                    let g = thought_acc.lock().unwrap();
                    if g.is_empty() {
                        None
                    } else {
                        Some(g.clone())
                    }
                };
                return Ok((
                    response,
                    ModelInfo {
                        provider: "Rainy API (Fallback)".to_string(),
                        model: "gemini-2.5-flash".to_string(),
                        plan_tier: "Pay-As-You-Go".to_string(),
                    },
                    thought,
                ));
            } else {
                println!("‚ùå Rainy API fallback failed");
            }
        } else {
            println!("‚ö†Ô∏è No Rainy API key configured");
        }

        // 3. Try Gemini BYOK fallback (only if API key is configured)
        if self
            .ai_provider
            .has_api_key("gemini")
            .await
            .unwrap_or(false)
        {
            let gemini_model = "gemini-3-flash-high";
            let thought_acc = std::sync::Arc::new(std::sync::Mutex::new(String::new()));
            let thought_writer = thought_acc.clone();
            println!(
                "üîÑ Trying Gemini BYOK fallback with model: {}",
                gemini_model
            );
            match self
                .ai_provider
                .execute_prompt(
                    &ProviderType::Gemini,
                    gemini_model,
                    prompt,
                    |_, _| {},
                    Some(move |chunk: StreamingChunk| {
                        if let Some(t) = chunk.thought {
                            if let Ok(mut g) = thought_writer.lock() {
                                g.push_str(&t);
                            }
                        }
                    }),
                )
                .await
            {
                Ok(response) => {
                    println!("‚úÖ Gemini BYOK fallback successful");
                    let thought = {
                        let g = thought_acc.lock().unwrap();
                        if g.is_empty() {
                            None
                        } else {
                            Some(g.clone())
                        }
                    };
                    return Ok((
                        response,
                        ModelInfo {
                            provider: "Google Gemini (Fallback)".to_string(),
                            model: gemini_model.to_string(),
                            plan_tier: "Free / BYOK".to_string(),
                        },
                        thought,
                    ));
                }
                Err(e) => {
                    println!("‚ùå AI Agent: Gemini fallback failed: {}", e);
                }
            }
        } else {
            println!("‚ö†Ô∏è No Gemini API key configured");
        }

        // 4. No providers available
        println!("‚ùå All providers failed or unavailable");
        Err("No AI providers available. Please configure an API key in settings.".to_string())
    }

    /// Build the prompt for AI task planning
    fn build_planning_prompt(&self, instruction: &str, context: &WorkspaceContext) -> String {
        format!(
            r#"You are an AI file management assistant for Rainy Cowork. Analyze the user's instruction and respond appropriately.

WORKSPACE INFO:
- Path: {}
- Files: {} | Folders: {}
- File types present: {}
- Sample files: {}

USER INSTRUCTION: "{}"

FIRST, classify the intent:
1. QUESTION - User is asking about files/folders (e.g., "What files are here?", "How many images?")
2. COMMAND - User wants to perform an operation (e.g., "Organize by type", "Delete old files", "Create a file")

Generate a JSON response with this structure:

For QUESTIONS:
{{
  "intent": "question",
  "answer": "Your detailed answer based on the workspace info above",
  "steps": []
}}

For COMMANDS:
{{
  "intent": "command",
  "answer": null,
  "steps": [
    {{
      "type": "organize_folder" | "move_file" | "delete_file" | "create_file" | "batch_rename",
      "path": "target path",
      "strategy": "by_type" | "by_date" | "by_extension" (for organize),
      "source": "source path" (for move),
      "destination": "dest path" (for move),
      "pattern": "rename pattern" (for batch_rename),
      "files": ["file1", "file2"] (for batch_rename),
      "content": "COMPREHENSIVE file content - see CONTENT QUALITY RULES below" (for create),
      "description": "Human readable description of this step"
    }}
  ],
  "warnings": ["any important warnings"],
  "requires_confirmation": true/false (true if any destructive operations)
}}

RULES:
1. For questions, provide helpful answers using the workspace info
2. For commands, only suggest operations that match the user's intent
3. Set requires_confirmation=true for any delete or modify operations
4. Include clear descriptions for each step
5. If unsure whether it's a question or command, treat it as a question
6. For "organize", prefer "by_type" strategy unless user specifies otherwise

CONTENT QUALITY RULES (CRITICAL - for create_file operations):
When creating file content, you MUST generate COMPREHENSIVE, PROFESSIONAL, and DETAILED content:

1. **Length**: Generate substantial content. A biography should be 2000+ words. A list should be complete. A report should be thorough. NEVER create stub or placeholder content.

2. **Research Quality**: Include accurate, detailed information. For biographies: include birth date, early life, career timeline, discography/filmography, achievements, quotes, personal life, legacy. For lists: include ALL items with details like dates, context, and significance.

3. **Structure**: Use proper formatting:
   - Clear sections with headers
   - Bullet points or numbered lists where appropriate  
   - Chronological ordering for timelines
   - Tables for comparative data

4. **Depth**: Go deep, not shallow. If asked about an artist's albums, list ALL albums with:
   - Release year
   - Number of tracks
   - Notable songs
   - Chart performance
   - Critical reception
   - Thematic content

5. **Professional Tone**: Write as if creating professional documentation or articles. Use proper grammar, complete sentences, and engaging prose.

6. **No Placeholders**: NEVER use placeholder text like "..." or "[continue here]" or "etc.". Complete the content fully.

7. **Maximum Output**: Use your full output capacity. Models support up to 65,000+ output tokens. A 20-line file when 2000+ lines are possible is UNACCEPTABLE.

Example of GOOD content for "Create a file about Lana Del Rey albums":
- Should list ALL 9 studio albums with full details
- Include EPs, singles, collaborations
- Add critical reception and awards
- Include quotes and notable lyrics
- At minimum 3000+ words

Respond ONLY with valid JSON, no other text."#,
            context.path,
            context.file_count,
            context.folder_count,
            context.file_types.join(", "),
            context.recent_files.join(", "),
            instruction
        )
    }

    /// Parse AI response into a TaskPlan
    fn parse_ai_response(&self, response: &str, instruction: &str) -> Result<TaskPlan, String> {
        // Extract JSON from response (handle markdown code blocks)
        let json_str = if response.contains("```json") {
            response
                .split("```json")
                .nth(1)
                .and_then(|s| s.split("```").next())
                .unwrap_or(response)
        } else if response.contains("```") {
            response.split("```").nth(1).unwrap_or(response)
        } else {
            response
        };

        // Parse JSON
        println!("üîç DEBUG: Raw AI response length: {} chars", response.len());
        if response.len() < 200 {
            println!("üîç DEBUG: Raw AI response: {}", response);
        } else {
            println!(
                "üîç DEBUG: Raw AI response (first 200 chars): {}...",
                &response[..200]
            );
        }

        // Handle completely empty or whitespace-only response
        let trimmed_response = json_str.trim();
        if trimmed_response.is_empty() {
            println!("‚ö†Ô∏è AI returned empty response, falling back to question intent");
            return Ok(TaskPlan {
                id: Uuid::new_v4().to_string(),
                instruction: instruction.to_string(),
                intent: TaskIntent::Question,
                answer: Some("I received your request but couldn't process it properly. Could you please rephrase your question or command?".to_string()),
                thought: None,
                model_used: None,
                steps: vec![],
                estimated_changes: 0,
                requires_confirmation: false,
                warnings: vec!["AI returned an empty response".to_string()],
                created_at: Utc::now(),
            });
        }

        // Try to parse JSON, with fallback for non-JSON responses
        let parsed: serde_json::Value = match serde_json::from_str(trimmed_response) {
            Ok(v) => v,
            Err(e) => {
                println!(
                    "‚ö†Ô∏è Failed to parse AI response as JSON: {}. Treating as plain text answer.",
                    e
                );
                // Fallback: treat the response as a plain text answer
                return Ok(TaskPlan {
                    id: Uuid::new_v4().to_string(),
                    instruction: instruction.to_string(),
                    intent: TaskIntent::Question,
                    answer: Some(response.to_string()),
                    thought: None,
                    model_used: None,
                    steps: vec![],
                    estimated_changes: 0,
                    requires_confirmation: false,
                    warnings: vec![],
                    created_at: Utc::now(),
                });
            }
        };

        // Parse intent (default to command for backwards compatibility)
        let intent = parsed
            .get("intent")
            .and_then(|i| i.as_str())
            .map(|s| match s {
                "question" => TaskIntent::Question,
                _ => TaskIntent::Command,
            })
            .unwrap_or(TaskIntent::Command);

        // Parse answer for questions
        let answer = parsed
            .get("answer")
            .and_then(|a| a.as_str())
            .map(|s| s.to_string());

        let mut steps = Vec::new();
        let mut warnings = Vec::new();
        let mut requires_confirmation = false;

        // Parse steps (only for commands)
        if intent == TaskIntent::Command {
            if let Some(steps_array) = parsed.get("steps").and_then(|s| s.as_array()) {
                for step_value in steps_array {
                    if let Some(step) = self.parse_step(step_value) {
                        if step.is_destructive() {
                            requires_confirmation = true;
                        }
                        steps.push(step);
                    }
                }
            }
        }

        // Parse warnings
        if let Some(warnings_array) = parsed.get("warnings").and_then(|w| w.as_array()) {
            for warning in warnings_array {
                if let Some(w) = warning.as_str() {
                    warnings.push(w.to_string());
                }
            }
        }

        // Override confirmation if explicitly set
        if let Some(confirm) = parsed
            .get("requires_confirmation")
            .and_then(|c| c.as_bool())
        {
            requires_confirmation = confirm;
        }

        Ok(TaskPlan {
            id: Uuid::new_v4().to_string(),
            instruction: instruction.to_string(),
            intent,
            answer,
            thought: None,    // Will be set by caller
            model_used: None, // Will be set by caller with actual model info
            steps: steps.clone(),
            estimated_changes: steps.len() as u32,
            requires_confirmation,
            warnings,
            created_at: Utc::now(),
        })
    }

    /// Parse a single step from JSON
    fn parse_step(&self, value: &serde_json::Value) -> Option<PlannedStep> {
        let step_type = value.get("type")?.as_str()?;
        let description = value
            .get("description")
            .and_then(|d| d.as_str())
            .unwrap_or("Execute operation")
            .to_string();

        match step_type {
            "organize_folder" => {
                let path = value.get("path")?.as_str()?.to_string();
                let strategy_str = value
                    .get("strategy")
                    .and_then(|s| s.as_str())
                    .unwrap_or("by_type");

                let strategy = match strategy_str {
                    "by_date" => OrganizeStrategy::ByDate,
                    "by_extension" => OrganizeStrategy::ByExtension,
                    "by_content" => OrganizeStrategy::ByContent,
                    _ => OrganizeStrategy::ByType,
                };

                Some(PlannedStep::OrganizeFolder {
                    path,
                    strategy,
                    description,
                })
            }
            "move_file" => {
                let source = value.get("source")?.as_str()?.to_string();
                let destination = value.get("destination")?.as_str()?.to_string();
                Some(PlannedStep::MoveFile {
                    source,
                    destination,
                    description,
                })
            }
            "delete_file" => {
                let path = value.get("path")?.as_str()?.to_string();
                Some(PlannedStep::DeleteFile { path, description })
            }
            "create_file" => {
                let path = value.get("path")?.as_str()?.to_string();
                let content = value
                    .get("content")
                    .and_then(|c| c.as_str())
                    .unwrap_or("")
                    .to_string();
                Some(PlannedStep::CreateFile {
                    path,
                    content,
                    description,
                })
            }
            "batch_rename" => {
                let files: Vec<String> = value
                    .get("files")?
                    .as_array()?
                    .iter()
                    .filter_map(|f| f.as_str().map(|s| s.to_string()))
                    .collect();
                let pattern = value.get("pattern")?.as_str()?.to_string();
                Some(PlannedStep::BatchRename {
                    files,
                    pattern,
                    description,
                })
            }
            _ => None,
        }
    }

    /// Execute a planned task
    pub async fn execute_plan(
        &self,
        plan_id: &str,
        on_event: Channel<AgentEvent>,
    ) -> Result<ExecutionResult, String> {
        let start_time = std::time::Instant::now();

        // Get the plan
        let plan = self
            .pending_plans
            .read()
            .await
            .get(plan_id)
            .cloned()
            .ok_or_else(|| format!("Plan not found: {}", plan_id))?;

        let task_id = plan_id.to_string();
        let total_steps = plan.steps.len();
        let mut completed_steps = 0;
        let mut total_changes = 0u32;
        let mut all_changes = Vec::new();
        let mut errors = Vec::new();

        // Execute each step
        for (index, step) in plan.steps.iter().enumerate() {
            // Emit step started
            let _ = on_event.send(AgentEvent::StepStarted {
                task_id: task_id.clone(),
                step_index: index,
                description: step.description().to_string(),
            });

            // Execute the step
            match self.execute_step(step).await {
                Ok(changes) => {
                    total_changes += changes.len() as u32;
                    all_changes.extend(changes.clone());
                    completed_steps += 1;

                    let _ = on_event.send(AgentEvent::StepCompleted {
                        task_id: task_id.clone(),
                        step_index: index,
                        changes,
                    });
                }
                Err(e) => {
                    errors.push(format!("Step {}: {}", index + 1, e));
                    let _ = on_event.send(AgentEvent::StepFailed {
                        task_id: task_id.clone(),
                        step_index: index,
                        error: e,
                    });
                }
            }

            // Update progress
            let progress = ((index + 1) as f32 / total_steps as f32 * 100.0) as u8;
            let _ = on_event.send(AgentEvent::Progress {
                task_id: task_id.clone(),
                progress,
                message: format!("Step {} of {} complete", index + 1, total_steps),
            });
        }

        // Remove from pending
        self.pending_plans.write().await.remove(plan_id);

        let duration_ms = start_time.elapsed().as_millis() as u64;
        let success = errors.is_empty();

        // Emit completion event
        if success {
            let _ = on_event.send(AgentEvent::Completed {
                task_id: task_id.clone(),
                total_changes,
            });
        } else {
            let _ = on_event.send(AgentEvent::Failed {
                task_id: task_id.clone(),
                error: errors.join("; "),
            });
        }

        Ok(ExecutionResult {
            task_id,
            success,
            total_steps,
            completed_steps,
            total_changes,
            changes: all_changes,
            errors,
            duration_ms,
        })
    }

    /// Execute a single step
    async fn execute_step(&self, step: &PlannedStep) -> Result<Vec<FileOpChange>, String> {
        match step {
            PlannedStep::OrganizeFolder { path, strategy, .. } => {
                let result = self
                    .file_ops
                    .organize_folder(path, strategy.clone(), false)
                    .await
                    .map_err(|e| e.to_string())?;
                Ok(result.changes)
            }
            PlannedStep::MoveFile {
                source,
                destination,
                ..
            } => {
                use crate::services::file_operations::MoveOperation;
                let ops = vec![MoveOperation {
                    source: source.clone(),
                    destination: destination.clone(),
                    on_conflict: ConflictStrategy::Rename,
                }];
                self.file_ops
                    .move_files(ops)
                    .await
                    .map_err(|e| e.to_string())
            }
            PlannedStep::DeleteFile { path, .. } => self
                .file_ops
                .safe_delete(vec![path.clone()])
                .await
                .map_err(|e| e.to_string()),
            PlannedStep::CreateFile { path, content, .. } => {
                // Use file manager to write
                let change = self
                    .file_manager
                    .write_file(path, content, None)
                    .await
                    .map_err(|e| e)?;

                Ok(vec![FileOpChange {
                    id: change.id,
                    operation: FileOpType::Create,
                    source_path: path.clone(),
                    dest_path: None,
                    timestamp: Utc::now(),
                    reversible: true,
                }])
            }
            PlannedStep::BatchRename { files, pattern, .. } => {
                use crate::services::file_operations::RenamePattern;
                let rename_pattern = RenamePattern {
                    template: pattern.clone(),
                    find: None,
                    replace: None,
                    counter_start: Some(1),
                    counter_padding: Some(3),
                };

                let previews = self
                    .file_ops
                    .batch_rename(files.clone(), rename_pattern, false)
                    .await
                    .map_err(|e| e.to_string())?;

                // Convert previews to changes
                let changes: Vec<FileOpChange> = previews
                    .into_iter()
                    .filter(|p| !p.has_conflict)
                    .map(|p| FileOpChange {
                        id: Uuid::new_v4().to_string(),
                        operation: FileOpType::Rename,
                        source_path: p.original,
                        dest_path: Some(p.new_name),
                        timestamp: Utc::now(),
                        reversible: true,
                    })
                    .collect();

                Ok(changes)
            }
            PlannedStep::ModifyFile {
                path, instruction, ..
            } => {
                // Read current content
                let current = self.file_manager.read_file(path).await.map_err(|e| e)?;

                // Use AI to transform content
                let new_content = {
                    let prompt = format!(
                        "Modify this file content according to the instruction.\n\nINSTRUCTION: {}\n\nCURRENT CONTENT:\n{}\n\nRespond with ONLY the new file content, nothing else.",
                        instruction, current
                    );

                    self.ai_provider
                        .execute_prompt(
                            &ProviderType::Gemini,
                            "gemini-2.5-flash",
                            &prompt,
                            |_, _| {},
                            None::<fn(StreamingChunk)>,
                        )
                        .await
                        .map_err(|e| e.to_string())?
                };

                // Write back
                let change = self
                    .file_manager
                    .write_file(path, &new_content, None)
                    .await?;

                Ok(vec![FileOpChange {
                    id: change.id,
                    operation: FileOpType::Move, // Using Move as "Modify" equivalent
                    source_path: path.clone(),
                    dest_path: None,
                    timestamp: Utc::now(),
                    reversible: true,
                }])
            }
            PlannedStep::AnalyzeContent { .. } => {
                // Analysis doesn't produce file changes
                Ok(Vec::new())
            }
        }
    }

    /// Get a pending plan by ID
    pub async fn get_plan(&self, plan_id: &str) -> Option<TaskPlan> {
        self.pending_plans.read().await.get(plan_id).cloned()
    }

    /// Cancel a pending plan
    pub async fn cancel_plan(&self, plan_id: &str) -> Result<(), String> {
        self.pending_plans
            .write()
            .await
            .remove(plan_id)
            .map(|_| ())
            .ok_or_else(|| format!("Plan not found: {}", plan_id))
    }

    /// Analyze workspace and generate suggestions
    pub async fn analyze_workspace(&self, path: &str) -> Result<WorkspaceAnalysis, String> {
        self.file_ops
            .analyze_workspace(path)
            .await
            .map_err(|e| e.to_string())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_step_destructive_check() {
        let delete_step = PlannedStep::DeleteFile {
            path: "/test".to_string(),
            description: "Delete".to_string(),
        };
        assert!(delete_step.is_destructive());

        let move_step = PlannedStep::MoveFile {
            source: "/a".to_string(),
            destination: "/b".to_string(),
            description: "Move".to_string(),
        };
        assert!(!move_step.is_destructive());
    }
}
